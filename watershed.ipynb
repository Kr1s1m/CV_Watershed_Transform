{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "475f4e18-7ef2-4ac5-aaa7-3932181bc8ec",
   "metadata": {},
   "source": [
    "# Watershed transform (digital image segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90d8b88-050f-4909-acd7-56ab697f176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3571bc1-75db-4843-b918-c02fbb75dd3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf5c80-b62a-4cd1-8541-401af6997ecd",
   "metadata": {},
   "source": [
    "In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n",
    "\n",
    "The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of geometry reconstruction algorithms like marching cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb53bf8-372d-4974-bd9a-ca395f349575",
   "metadata": {},
   "source": [
    "## Task definition and mathematical model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a839524-70a5-43ef-91ee-de5b523835e0",
   "metadata": {},
   "source": [
    "As stated above we are trying to find the partitioning of a digital image into multiple regions (sets of pixels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b18bda-d8fb-49c4-ab9e-28983da8def7",
   "metadata": {},
   "source": [
    "Let $R$ represent the entire spatial region occupied by an image. We may view image segmantation as a process that partitions $R$ into $n$ subregions, $R_1$, $R_2$,..., $R_n$ such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53e49d-ef05-4fc7-bd5d-58392ce7e378",
   "metadata": {},
   "source": [
    "(a) $\\bigcup_{i=1}^nR_i=R$ $\\iff$ the segmentation must be complete - every pixel must be in a region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9d27d-0918-47ed-bec4-52180b19b844",
   "metadata": {},
   "source": [
    "(b) $R_i$ is a connceted set, for $i = 0, 1, 2, ..., n$ $\\iff$ points in a region must be connected in a predefined sense (e.g., 8-connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69581f18-83ad-4b9a-bf44-123a869cf74d",
   "metadata": {},
   "source": [
    "(c) $R_{i} \\cap R_{j} = \\varnothing$, for all i and j, $i \\neq j$ $\\iff$ different regions must be $\\textit{disjoint}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696aa8e2-48ce-4e18-9689-7aeb2a0b938a",
   "metadata": {},
   "source": [
    "(d) $Q(R_i) = \\textnormal{TRUE}$, for $i = 0, 1, 2, ..., n$ $\\iff$ pixels in a region must statisfy some property defined by prediacte $Q$ (e.g., intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d8243-204a-45cc-96c7-38d461795da5",
   "metadata": {},
   "source": [
    "(e) $Q(R_{i} \\cup R_{j}) = \\textnormal{FALSE}$, for any $\\textit{adjacent}$ regions $R_i$ and $R_j$ $\\iff$ $\\textit{adjacent}$ regions must be different in the sense of predicate $Q$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c190c-bf40-4679-9ee6-02ec62fec713",
   "metadata": {},
   "source": [
    "where $Q(R_k)$ is a logical predicate defined over the points in set $R_k$ and $\\varnothing$ is the null set. The symbols $\\cup$ and $\\cap$ represent set union and intersection, respectively. Two regions $R_i$ and $R_j$ are said to be $\\textit{adjacent}$ if their union forms a connected set. If the set formed by the union of two regions is not connected, the regions are said to be $\\textit{disjoint}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23468f6-5755-4884-ab68-626b8c7a3f86",
   "metadata": {},
   "source": [
    "Thus, we see that the fundamental problem in segmentation is to partition an\n",
    "image into regions that satisfy conditions (a) — (e). Segmentation algorithms\n",
    "for monochrome images generally are based on one of two basic categories dealing\n",
    "with properties of intensity values: $\\textit{discontinuity}$ and $\\textit{similarity}$. In the first category,\n",
    "we assume that boundaries of regions are sufficiently different from each other, and\n",
    "from the background, to allow boundary detection based on local discontinuities in\n",
    "intensity. $\\textit{Edge-based}$ segmentation is the principal approach used in this category.\n",
    "$\\textit{Region-based}$ segmentation approaches in the second category are based on partitioning an image into regions that are similar according to a set of predefined criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37303e0e-ce68-4203-9420-993e0ec349d6",
   "metadata": {},
   "source": [
    "## Optimum Global Thresholding Using Otsu's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5650c-2266-4114-9eeb-bc2eb2825a44",
   "metadata": {},
   "source": [
    "Thresholding may be viewed as a statistical-decision theory problem whose objective is to minimize the average error incurred in assigning pixels to two or more\n",
    "groups (also called classes). This problem is known to have an elegant closed-form\n",
    "solution known as the $\\textit{Bayes decision function}$. The solution is\n",
    "based on only two parameters: the probability density function (PDF) of the intensity levels of each class, and the probability that each class occurs in a given application. Unfortunately, estimating PDFs is not a trivial matter, so the problem usually\n",
    "is simplified by making workable assumptions about the form of the PDFs, such as\n",
    "assuming that they are Gaussian functions. Even with simplifications, the process\n",
    "of implementing solutions using these assumptions can be complex and not always\n",
    "well-suited for real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b06f54-a2ef-4f3a-8977-44070b04370a",
   "metadata": {},
   "source": [
    "The approach in the following discussion, called Otsu’s method (Otsu 1979), is\n",
    "an attractive alternative. The method is optimum in the sense that it maximizes the between-class variance, a well-known measure used in statistical discriminant analysis. The basic idea is that properly thresholded classes should be distinct with respect to the intensity values of their pixels and, conversely, that a threshold giving the\n",
    "best separation between classes in terms of their intensity values would be the best\n",
    "(optimum) threshold. In addition to its optimality, Otsu’s method has the important\n",
    "property that it is based entirely on computations performed on the histogram of an\n",
    "image, an easily obtainable 1-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decaf8ef-f7f4-4bc9-8bb3-69784497b3cb",
   "metadata": {},
   "source": [
    "Otsu's method, named after Nobuyuki Otsu (大津展之, Ōtsu Nobuyuki), is used to perform automatic image thresholding. In the simplest form, the algorithm returns a single intensity threshold that separate pixels into two classes, foreground and background. This threshold is determined by minimizing intra-class intensity variance, or equivalently, by maximizing inter-class variance. Otsu's method is a one-dimensional discrete analogue of Fisher's discriminant analysis, is related to Jenks optimization method, and is equivalent to a globally optimal k-means performed on the intensity histogram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c327bfa-6031-4ad1-94db-5de9437cc3b6",
   "metadata": {},
   "source": [
    "<img src=\"understanding_otsu/screenshot_histogram_otsu.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edba27-1c1a-4e6b-b464-214eb33db2ce",
   "metadata": {},
   "source": [
    "The algorithm exhaustively searches for the threshold that minimizes the intra-class variance, defined as a weighted sum of variances of the two classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13373191-4941-49d2-8dca-001f3e64f65e",
   "metadata": {},
   "source": [
    "$\\sigma_{w}^2(t) = \\omega_{0}(t)\\sigma_{0}^2(t) + \\omega_{1}(t)\\sigma_{1}^2(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8305d8-d0e7-4946-857c-8a02522c62c0",
   "metadata": {},
   "source": [
    "Weights $\\omega_0$ and $\\omega_1$ are the probabilities of the two classes seperated by a threshold $t$ and $\\sigma_{0}^2$ amd $\\sigma_{1}^2$ are variances of the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc294f-ad2e-446b-b17d-3cf794f28216",
   "metadata": {},
   "source": [
    "The class probablitiy $\\omega_{\\{0, 1\\}}(t)$ is computed from the $L$ bins of the histogram:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d253b24-03c9-47c5-8a92-2a0601224e0a",
   "metadata": {},
   "source": [
    "$\\omega_{0}(t) = \\sum_{i=0}^{t-1}p(i)$,  $\\omega_{1}(t) = \\sum_{i=t}^{L-1}p(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b63f0-87fc-4270-9a00-f26f85486576",
   "metadata": {},
   "source": [
    "For 2 classes, minimizing the intra-class variance is equivalent to maximizing inter-class variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311ad37-d98e-4f5b-972b-79b189006641",
   "metadata": {},
   "source": [
    "$\\sigma_{b}^2(t) = \\sigma^2 - \\sigma_{w}^2(t) = \\omega_{0}(t)(\\mu_{0}-\\mu_{T}^2) + \\omega_{1}(t)(\\mu_{1}-\\mu_{T}^2) = \\omega_{0}(t)\\omega_{1}(t)[\\mu_{0}(t)-\\mu_{1}(t)]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beccf6f-bbdb-445a-9bd3-af7dfa207268",
   "metadata": {},
   "source": [
    "which is expressed in terms of class probablities $\\omega$ and class means $\\mu$, where: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25ac89-6ac3-49f7-81e2-e5db3f7ec762",
   "metadata": {},
   "source": [
    "$\\mu_{0}(t) = \\frac{\\sum_{i=0}^{t-1}ip(i)}{\\omega_{0}(t)}$, $\\mu_{1}(t) = \\frac{\\sum_{i=t}^{L-1}ip(i)}{\\omega_{1}(t)}$, $\\mu_{T} = \\sum_{i=0}^{L-1}ip(i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d9dcf-2fae-42be-8856-55b244e22418",
   "metadata": {},
   "source": [
    "The following relations can easily be verified:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb867db-9dd7-42f8-a495-ae2ef03af9ea",
   "metadata": {},
   "source": [
    "$\\omega_{0}\\mu_{0} + \\omega_{1}\\mu_{1} = \\mu_{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382e76d-932e-478b-ba6e-b62722951aa3",
   "metadata": {},
   "source": [
    "$\\omega_{0} + \\omega_{1} = 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cbbb5-a413-4d82-85c7-455614bcc8ab",
   "metadata": {},
   "source": [
    "The class probabilities and class means can be computed iteratively. This idea yields an effective algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae771d-90f3-47e7-875b-1afbdab29c81",
   "metadata": {},
   "source": [
    "(1) Compute histogram and pobabilities of each intensity level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546c1d3-a352-4165-a5e2-ff8f7c0f82f3",
   "metadata": {},
   "source": [
    "(2) Set up initial $\\omega_{i}(0)$ and $\\mu_{i}(0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5f0eb-a70c-4d42-a298-06ed4b7d6bed",
   "metadata": {},
   "source": [
    "(3) For all possible thresholds $t = 1, ...,$ $maximum$ $intensity$ do:\n",
    "        (1) Update $\\omega_{i}$ and $\\mu_{i}$\n",
    "        (2) Compute $\\sigma_{b}^{2}(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d76f6-4514-4e3e-9759-3d61c7a4a27f",
   "metadata": {},
   "source": [
    "(4) Desired threshold $t^{*}$ corresponds to the maximum $\\sigma_{b}^2(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a3de6-e593-4dbf-abdf-4ceea533d58b",
   "metadata": {},
   "source": [
    "Once $t^{*}$ has been obtained, input image $f(x, y)$ is segmented as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a85c0-c18a-40de-a8f2-c5a44cfdd27d",
   "metadata": {},
   "source": [
    "$g(x, y)= \n",
    "\\begin{cases}\n",
    "    1,              & f(x, y) > t^{*}\\\\\n",
    "    0,              & f(x, y) \\leq t^{*}\n",
    "\\end{cases}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17dbc0bc-8826-43bd-b241-170de5f74718",
   "metadata": {},
   "source": [
    "<img src=\"understanding_otsu/original_f(x,y).jpg\"/> $\\implies$ <img src=\"understanding_otsu/binary_g(x,y).jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963c3dc-0abb-4dcd-b1bc-9919d7d19edd",
   "metadata": {},
   "source": [
    "## Watershed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d68b5ff-84b7-433c-986b-c002bfe01170",
   "metadata": {},
   "source": [
    "In the study of image processing, a watershed is a transformation defined on a grayscale image. The name refers metaphorically to a geological watershed, or drainage divide, which separates adjacent drainage basins. The watershed transformation treats the image it operates upon like a topographic map, with the brightness of each point representing its height, and finds the lines that run along the tops of ridges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e1fcd7c-3220-44df-b439-b14523f8eede",
   "metadata": {},
   "source": [
    "<img src=\"understanding_watershed/mathworks_syntethic_dark_blobs.jpg\"/> $\\iff$ <img src=\"understanding_watershed/mathworks_syntethic_dark_blobs_topographic_surface.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f91d47-7ff4-42db-a8fc-a44949ebdef9",
   "metadata": {},
   "source": [
    "Any grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys. You start filling every isolated valley (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys with different colors will start to merge. To avoid that, you build barriers (dams) in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created give you the segmentation result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3784502a-0850-419e-a78f-a383aa0debaf",
   "metadata": {},
   "source": [
    "<img src=\"understanding_watershed/book_dam_construction_via_dilation.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568fbf6b-d82e-4128-ba09-edd5bd797a9f",
   "metadata": {},
   "source": [
    "The watershed segmentation working on a grayscale image was introduced by F. Meyer in 1994. During sucessive flooding of the grey value relief, watersheds with adjacent catchment basins are constructed. This flooding process is performed on the gradient image, i.e. the basins should emerge along the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583dfe0-37fa-4d61-a8f0-7c4d3ed279d8",
   "metadata": {},
   "source": [
    "(1) A set of pixels where the flooding should start are chosen. Each is given a different label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbbaab-8dea-4451-9c3e-7d48eb444af3",
   "metadata": {},
   "source": [
    "(2) The neighbouring pixels of each marked area are inserted into a priority queue with a priority level corresponding to the gradient magnitude of the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee671a-92a9-4146-807e-a76f1e2ac60b",
   "metadata": {},
   "source": [
    "(3) The pixel with the lowest priority level is extracted from the priority queue. If the neighbours of the extracted pixel that have already been labeled all have the same label, then the pixel is labeled with their label. All non-marked neighbours that are not yet in the priority queue are put into the priority queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dbd99-e275-4bbd-a88c-e33f6978c9cf",
   "metadata": {},
   "source": [
    "(4) Repeat (3) until the priority queue is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b71283-c88f-4337-b1fa-6840efd04bdd",
   "metadata": {},
   "source": [
    "(5) The non-labeled pixels are the watershed lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f5b21fc-a132-4ac7-886d-c42fe89b1393",
   "metadata": {},
   "source": [
    "<img src=\"understanding_watershed/gradient_flooding.jpg\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a779908-7d7b-409f-876b-716b86462885",
   "metadata": {},
   "source": [
    "But this approach gives you oversegmented result due to noise or any other irregularities in the image. Oversegmentation occurs because every regional minimum, even if tiny and insignificant, forms its own catchement basin. Either the image must be pre-processed or the regions must be merged on the basis of a similarity criterion afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79dea59-2065-4915-869a-cc95490977ce",
   "metadata": {},
   "source": [
    "Example of watershed oversegmenation caused by local minima present in the raw unprocessed image of steel grains:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4cade56-bdb9-486b-9352-6664aaca5a0d",
   "metadata": {},
   "source": [
    "<img src=\"understanding_watershed/mathworks_steel_grains.jpg\"/> $\\implies$ <img src=\"understanding_watershed/mathworks_steel_grains_oversegmentation.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80387ab4-6384-4cd4-a8f2-7db7f7f6e1f4",
   "metadata": {},
   "source": [
    "## The use of markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f570670-0a64-415b-8ec4-4f3151b154d1",
   "metadata": {},
   "source": [
    "Direct application of the watershed segmentation algorithm in the form discussed\n",
    "in the previous section generally leads to oversegmentation, caused by noise and\n",
    "other local irregularities of the gradient. As the example above illustrates, over-segmentation\n",
    "can be serious enough to render the result of the algorithm virtually useless. In this\n",
    "case, this means a large number of segmented regions. A practical solution to this\n",
    "problem is to limit the number of allowable regions by incorporating a preprocessing stage designed to bring additional knowledge into the segmentation procedure. An approach used to control oversegmentation is based on the concept of markers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1066ab5-cb1b-49be-a215-df389c1cb19e",
   "metadata": {},
   "source": [
    "A marker is a connected component belonging to an image. We have internal\n",
    "markers, associated with objects of interest, and external markers, associated with\n",
    "the background. A procedure for marker selection typically will consist of two principal steps: 1) preprocessing; and 2) definition of a set of criteria that markers must satisfy. Part of the problem that led to the oversegmented result in the above example is the large number of potential minima. Because of their size, many of these minima are irrelevant detail. An effective method for minimizing the effect of small spatial detail is to filter the image with a smoothing filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9f4a6-64e6-4f9f-a911-ccd8ed6ef07e",
   "metadata": {},
   "source": [
    "Suppose that we define an internal marker as 1) a region that is surrounded by\n",
    "points of higher “altitude”; 2) such that the points in the region form a connected\n",
    "component; and 3) in which all the points in the connected component have the\n",
    "same intensity value. The external markers effectively partition the image into regions,\n",
    "with each region containing a single internal marker and part of the background.\n",
    "The problem is thus reduced to partitioning each of these regions into two: a single\n",
    "object, and its background. We can bring to bear on this simplified problem\n",
    "the segmentation techniques discussed in earlier sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4d3ca-ae31-45cb-9bc7-a8fa38084973",
   "metadata": {},
   "source": [
    "OpenCV implemented a marker-based watershed algorithm where you specify which valley points are to be merged and which are not. It is an interactive image segmentation. We give different labels for the objects we know. Label the region which we are sure of being the foreground or object with one color (or intensity), label the region which we are sure of being background or non-object with another color and finally the region which we are not sure of anything, label it with 0. That is our marker. Then apply watershed algorithm. Then our marker will be updated with the labels we gave, and the boundaries of objects will have a value of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcf2bd-4b19-42ca-8932-43759742e5f3",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf6d5f-43d5-4963-9075-2f223f52f1d4",
   "metadata": {},
   "source": [
    "We will first define a general parametric procedure, leveraging the cv2 library, which will allow us to easily experiment with different ways of approaching pre-processing and marker selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0046a5-cf53-48ad-98a9-4442b03e5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watershed_transform(img, blur, dilation):\n",
    "    #convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    #find an approximate binary segmentation using Otsu's optimal threshold binarization\n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "    #remove noise using morphological opening\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations = 2)\n",
    "    #sure background area\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations = 3)\n",
    "    # find sure foreground area\n",
    "    dist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "    ret, sure_fg = cv.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "    #find unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg, sure_fg)\n",
    "    #marker labeling\n",
    "    ret, markers = cv.connectedComponents(sure_fg)\n",
    "    #add 1 to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "    #mark the region of unknown with zero\n",
    "    markers[unknown==255] = 0\n",
    "    #perform the watershed algorithm which will mark the boundary with -1\n",
    "    markers = cv.watershed(img, markers)\n",
    "    #set the boundary pixels to red on the original image\n",
    "    img[markers == -1] = [255, 0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
